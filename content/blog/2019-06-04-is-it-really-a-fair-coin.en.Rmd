---
title: 'Is it really a fair coin? '
author: Mehmet Oner Yalcin
date: '2019-06-04'
slug: is-it-really-a-fair-coin (Bayesian Detective Bureau)
categories:
  - Probability
tags:
  - R
description: ''
featured: ''
featuredalt: ''
featuredpath: ''
linktitle: ''
type: post
---


Unlike any other detectives, he was not not very much into chasing leads out in the wild. He never enjoyed running after serial killers, or going into adventures like Indiana Jones. He was way way too lazy to become a real detective. But let's make no mistake, like any other detective he loves solving mysteries. Only problem is that he never liked rain snow or cold, and preferred being a lazy cat be an Indiana Jones. There were two things he hated in the life, getting soaked in rain, and bagged tea. 

He was the Sherlock Holmes of pen, paper and few lines of computer code. Since he's an unusal detective, his clients were too unusal. They vary from those who lost a lot of money in gambling and want to prove casinos are cheating, to high profile scientists tring to decide when to launch a spacecraft, or football coaches looking for a new defensive midfielder but not trusting his own scouts. 


It was a another miserable morning, rain never stopped for a moment since he woke up. That's one of the days you want to spend under a blanket and do simply nothing but only sleep. Nevertheless he came to the rooftop tiny room that he call his office, in reality it was more of an attic. Anyways he doesn't need anything fancy. There were few things in the room; few colorful pens, a deck of papers, some old newspapers, PC, desk and a nice small tea cup with few loose tea storages. Possibly the kettle was the most expensive item there. One can even claim his tiny attic is pretty big compared to the area he actively uses. After all, what is the point of having a big office if you only use 1.5 meter square?

As he always does, his brewed the tea and was enjoying it with sesame bagel. Brewing tea is one of the few things he was never lazy about, ironically he thought bagged tea is for lazy people. 

After finishing his modest breakfast, he briefly checked papers. They were all telling about two airline crashes in the last three days. These crashes already caused many conspiracy theories. He didn't take it immediately serious as he knew that the same papers also tell him Mars' gravity would change his love life, but he knows the bagel he just ate two mins ago has more gravitational force than Mars, considering the distance between Mars and him and bagel and him. Then he went back to the newspaper article, thinking having plane crashes in three days apart is possibly very normal, it's just a poisson distribution he told tpo himself, but to be sure he needs to look at plane crash data. Possibly nothing to worry about he said. He decided to check this later when he has some free time from laziness. 

He couldn't finish reading the rest of the newspaper as the rain sound hitting on roof and warm office seat made him sleepy, and didn't take a min to completely switch to the other side.  

After a while, his door was knocked and a frustrated looking man in wet clothes came in, detective woke up briefly, expecting this frustrated man would go away as this was his main strategy when his sleep was cut in half. He always prefers frustrated looking clients in dreams to actual frustrated looking clients, as with the former one you can deal with them in the way you want, latter one takes a bit more effort especially when they are frustrated and wet. 

After a long silence, detective finally accepted that his denial and ever increasing silence wouldn't help to make him go away, as the frustrated looking gentleman was getting wetter, more frustrated. As silence grew, silence become like a third man sitting next to them and only listening  but not talking. He doesn't feel comfortable. There are two things her hates in the life, people doesn't uncomfortably participate in conversations and bagged tea. He cleaned the last bit of sesame seeds in his teeth. He made habit of cleaning his teeth, as sesam was always the issue after enjoying a morning bagel, resulting in many failed dates:

\- yes?...Mr..?

\- Is this the bayesian detective bureau? asked wet frustated looking man.  

\- ...

\- Not sure I'm in the correct place, Errmm..I'm looking for investigator Independent Variable. 

Yes, not a conventional name but that's exactly what was written in his business card, *Mr. Independent Variable - Private Investigator*

\- Yes, it's ... me, Mr...?
 
while waking up. frustrated man didn't wait for detective to ask why he was here and immediately went into the subject: 

\- My name is , well, doesn't really matter.. I am an investigator from a regulatory body to make sure casinos in the country are relatively fair to customers, well... at least they don't do something hidden we don't know. There is this particular casino with a bit dodgy name called *Casino Trust* and they run basically a simple game very similar to a coin flip. We are receiving a lot of complaints from the casino customers that the digital coin flip is biased. I went yesterday to casino and they provided me the last 50000 flip results, and honestly it looks coin flips are fair to me. Almost half of the flips was tails and and the other half was heads. I cannot really say that it is a biased game. However we have excessive amount of complaints from clients so we have to really dig deep. I am wondering detective, can you also have a look at it? 

Our detective was already awake after he heard coin flips. He has some similarity to great mathematician Paul Erdős. You feed Erdős with coffee and clean laundry and you get best academic papers as output. Our detective was no short of that, feed him with questions about coin flips or dice games and you get his attention and an elegant solution and getting his attention was sometimes even harder than make a regular camel win in chess against Kasparov. He turned on his PC and started looking at the coin flips data:

```{r, include=FALSE}
library(reticulate)
use_python("/Users/mya03/dev/junk/venv3/bin/python3")

set.seed(42)

# Function reference: https://stephens999.github.io/fiveMinuteStats/simulating_discrete_chains_1.html

run.mc.sim <- function( P, num.iters = 50 ) {
    
    # number of possible states
    num.states <- nrow(P)
    
    # stores the states X_t through time
    states <- numeric(num.iters)
    
    # initialize variable for first state 
    states[1] <- 1
    
    for(t in 2:num.iters) {
        
        # probability vector to simulate next state X_{t+1}
        p  <- P[states[t-1], ]
        
        ## draw from multinomial and determine state
        states[t] <- which(rmultinom(1, 1, p) == 1)
    }
    return(states)
}

# Markov Chain Transition Matrix
p<- matrix(c( 0.6, 0.4, 
                0.4, 0.6), nrow=2, ncol=2)

casino <- run.mc.sim(p,num.iters = 50000) -1 
```


```{python, include=FALSE}
import numpy as np

dataset  = np.array([int(x) for x in r.casino])

```

In the dataset 0 represents **tails** and 1 represent **heads** said no name gentleman.

```{python}
print(dataset)
```

It has 50000 games :
```{python}
len(dataset)
```

And as the no name gentlemen said, the $P(H)$ is pretty close to $0.5$ which indeed looks like a fair coin

```{python}
dataset.mean()
```


Detective puzzled a bit for a moment, as game results seems pretty regular and fair. After all it's hard to blame the casino for not being fair if the 50000 game if dataset shows $P(HEADS)$ ~ $P(TAILS)$ ~ $0.5$

Detective draw a quick diagram on the paper, as he loves to think visually and pen and paper are generally the best tools while thinking on the problem:

\- This is a state diagram of a fair coin, said detective. And in a fair coin getting HEADS in the next game is independent from the outcome of the previous game, $P(HEADS|TAILS) = P(HEADS|HEADS) = 0.5$ meaning probability events are independent. 

```{r, include=FALSE, fig.width=5, fig.height=5}
# library(diagram)
# p_fair<- matrix(c( 0.5, 0.5,
#                 0.5, 0.5), nrow=2, ncol=2)
# plotmat(p_fair, relsize = 0.6, lwd=2, name=names, box.prop = 0.9, curve = 0.01, self.cex = 0.6)
```
![](/blog/2019-06-04-is-it-really-a-fair-coin.en_files/Screen Shot 2019-06-04 at 15.05.46.png)


He looked a bit closer to the diagram, and started talking to himself as he always does when he is focused,

\- Himm, we know at least: $$P(HEADS|TAILS) + P(HEADS|HEADS) =  P(TAILS|HEADS) + P(TAILS|TAILS)$$.

  and if we assume coin is fair then
  
- $P(HEADS|TAILS) = 0.5 \times 0.5 = 0.25$ 
- $P(HEADS|HEADS) = 0.5 \times 0.5 = 0.25$ 
- $P(TAILS|HEADS) = 0.5 \times 0.5 = 0.25$
- $P(TAILS|TAILS) = 0.5 \times 0.5 = 0.25$

The gentlemen was trying to follow the detective, thinking about independency of coin flips. Right at that point he remembered one of customer complaints. He said:

\ - Number of customers said they believed if the previous game was HEADS, they think the next game would be more likely HEADS too. 

\- Are you serious?! detective shouted. You are telling me events are not independent? That would fundamentally  changes our assumption. Gentleman puzzled :

\- I don't get it fully, if the events are not independent how is it possible $P(HEADS) = P(TAILS) = 0.5$?

Detective *Independent Variable* was looking excited, and rewote the assumptions:

\- We all need to satisfy only one conditions to keep $P(HEADS) = P(TAILS) = 0.5$, an that is ensuring transition probabilites equal, they don't need to be 0.5 but they must be equal:

 $$P(HEADS|TAILS) = P(TAILS|HEADS)$$

and let's say if our diagram changes, and the game outcome becomes stickier due to transition probabilities, meaning if previous result is HEADS this game result is also more likely be HEADS, we would still get $P(HEADS) = P(TAILS) = 0.5$ as transition probabilites are equal. Just our dataset would have different number of consecutive same faces. If the game becomes stickier, than we would see more consecutive series like (TAIL>TAIL>TAIL) than usual.   

![Stickier Transition State Diagram](/blog/2019-06-04-is-it-really-a-fair-coin.en_files/Screen Shot 2019-06-04 at 15.28.28.png)

Wet and frustrated gentleman started to get the idea now. But still he has questions:

\- I think I got what you mean, but how can we prove this is the case?

Detective thought a min, and said:

\- I think there are number of ways we can prove it. But I'd like to explore both visually and analitically. In a fair coin case the possibility of transitioning from one state to another (i.e. HEADS > TAILS > HEADS  or TAILS > HEADS > TAILS) is =  $0.5(transition\ in) \times 0.5(transition\ out) = 0.25$. similarly having two consecutive same face is  $0.5 (transition\ in) \times 0.5 (same face) \times 0.5 (transition\ out) = 0.125$ as all these events are independent. If we plot the histogram of coins series where we get the same coin consutively in a fair coin we should see this diminishing factor of $0.5$ with each number. In our loaded coin example this would be different. Diminishing factor would be different and more gradual as it's more sticky. Transitioning from one state to another would be $0.1(transition in) \times 0.1(transition out) = 0.01$  and two conscutive same faces would be $0.1(transition in) \times 0.9(same face) \times 0.1(transition out) = 0.009$.


In general formula for $n$ number of consecutive faces would be; $$P(n\ Consecutive\ faces) = P(transition\ in) \times P(same\ face)^{(n-1)} \times P(transition\ out)$$


Assuming $P(transition\ in) = P(transition\ out)$ our formula would be $$P(n\ Consecutive\ faces) = P(transition)^2 \times P(same\ face)^{(n-1)} $$


I'll generate an artifical 50000 fair coin flips to see it in action:

```{python}
import numpy as np
np.random.seed(42)

fair_coin = np.random.randint(0,2, 50000)
fair_coin[:20]
```

above series would be summarized as  $1T, 1H, 3T, 1H, 3T, 1H, 4T...$ etc 

I will create exactly that series above. 

```{python}
from itertools import groupby
series= np.array([len(list(x)) for key, x in groupby(fair_coin)])
series[:10]
```

Let's look at the histogram of consecutive series:

```{r, warning=FALSE, message=FALSE}
hist(py$series, bins= max(py$series) -1, xlab = "Consecutive series", 
     ylab = "Number of occurences", main = "Histogram of consecutive # of same faces (fair coin)")
```

Let's also calculate the ratio of consequtive series, remember 1 consecutive means TAILS>TAILS or HEADS>HEADS.  and $P(H|T)\times P(H) \times P(T|H) = 0.125$

```{r}
table(py$series)/50000
```


Our detective continued talking:

\- let's look at our not so fair coin model that depends on the previous state (current state depends only the last state not ALL previous states)

![Sticky coin, P(TAILS|TAILS) = P(HEADS|HEADS) = 0.9](/blog/2019-06-04-is-it-really-a-fair-coin.en_files/Screen Shot 2019-06-05 at 09.43.53.png)

```{r, include=FALSE}
p_unfair <- matrix(c( 0.9, 0.1, 
                      0.1, 0.9), nrow=2, ncol=2)

unfair <- run.mc.sim(p_unfair, num.iters = 50000) -1
```

```{r}
unfair[1:50]
length(unfair)
mean(unfair)
```


```{python}
from itertools import groupby
unfair_series= np.array([len(list(x)) for key, x in groupby(np.array(r.unfair))])
unfair_series[:10]
```


```{r warning=FALSE, message=FALSE}
table(py$unfair_series)/50000
# 
hist(py$unfair_series, bins= max(py$unfair_series) -1, xlab = "Consecutive series",
     ylab = "Number of occurences in unfair coin", main = "Histogram of consecutive # of same faces (unfair coin)")
```

After looking at the result of histogram and the table, no name frustrated looking gentleman grasped the idea. He was no longer frustrated nor wet. Excitingly he said:
 - Right! Makes very much sense now. I think we can even quantify the transition matrix for the casino coin! All we need to take the square root of the first in the table. That should give us the probaility of transitioning from HEADS to TAILS or TAILS to HEADS. If it is different than 0.5 we know it's not a fair coin and we can find how sticky it is.
 
Detective nodded his head:

\- Exactly! This is interesting since the coin is both fair and unfair at the same time. It is a ***HeisenCoin***. I coined this term! Anyways, shall we look at the casino dataset? 


Satisfied looking no name dry gentleman was not very happy with this silly joke but didn't want to say it outloud. Simply smiled and agreed to look at casino data. There are two things detective hated in the life, clients doesn't laugh to his brilliant jokes and bagged tea. But he kept his calm and loaded the casino dataset for analysis:

```{python}
dataset[:20]
casino_series = np.array([len(list(x)) for key, x in groupby(dataset)])
casino_series[:10]
```

```{r}
table(py$casino_series)/50000
```

\- The first term is $0.16$, this means we indeed DO have an unfair game and $P(HEAD|TAILS)$ or $P(TAILS|HEAD) = 0.4$. Detective draw transition state diagram on a piece of paper. 

![Casino Coin Transition State Diagram](/blog/2019-06-04-is-it-really-a-fair-coin.en_files/Screen Shot 2019-06-05 at 10.23.08.png)


Gentleman was thrilled:
\ -  Yes indeed! The second term, two consecutive faces TAILS>TAILS or HEADS>HEADS must be then $P(2) = P(transition)^2 \times P(same\ face) =  0.4^2 \times 0.6 = 0.096$ and indeed true, our simulation result is$ $0.9632$! Three consecutive faces must be $P(3) = 0.4^2 \times 0.6^2 = 0.0576$ very close to our simulation findings of $0.597$. Thank you detective! you are the best! 
 
Our detective was going to give a bit more information but thrilled looking no name gentleman was in hurry, as he was running to quit. Detective just said one last sentence before he leaves the office:

\- Next time if you are suspicious that an event depends on its immediate previous state then check out Markov Chains first. and ... yes you are welcome...




#### NOTES

A simple python code that populates an unfair (sticky) markov coin:

```
import random
import numpy as np 

def markov_coin(n, p=0.5):
    """ 
    Populate n samples of coin flips with stickiness probability of p
    p = 0.5 is a fair coin, p>0.5 is a sticky coin, p<0.5 is a hot potato coin.
    Sticky coin tends to stay in the same state more, and hot potato coin is eager to 
    change state
    """
    
    # Initialize list
    arr = list()
    arr.append(random.random()>0.5)
    
    for i in range(1,n):
        if random.random() < p:
            arr.append(arr[i-1])
        else:
            arr.append(not arr[i-1])
    return np.array(arr)
    
```


Markov chain model in `R`. Slightly more complete example compared to pythona s it takes transition matrix of $P$

```
# Function reference: https://stephens999.github.io/fiveMinuteStats/simulating_discrete_chains_1.html

run.mc.sim <- function(P, num.iters = 50 ) {
    
    # number of possible states
    num.states <- nrow(P)
    
    # stores the states X_t through time
    states <- numeric(num.iters)
    
    # initialize variable for first state 
    states[1] <- 1
    
    for(t in 2:num.iters) {
        
        # probability vector to simulate next state X_{t+1}
        p <- P[states[t-1], ]
        
        ## draw from multinomial and determine state
        states[t] <- which(rmultinom(1, 1, p) == 1)
    }
    return(states)
}

# Markov Chain Transition Matrix
P<- matrix(c( 0.6, 0.4, 
              0.4, 0.6), nrow=2, ncol=2)
```

