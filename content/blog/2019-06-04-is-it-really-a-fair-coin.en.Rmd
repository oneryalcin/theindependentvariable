---
title: 'Is it really a fair coin? '
author: Mehmet Oner Yalcin
date: '2019-06-04'
slug: is-it-really-a-fair-coin (Bayesian Detective Bureau)
categories:
  - Probability
tags:
  - R
description: ''
featured: ''
featuredalt: ''
featuredpath: ''
linktitle: ''
type: post
---


Unlike any other detectives, he was not not very much into chasing leads out in the wild. He never enjoyed running after serial killers, or going into adventures like Indiana Jones. He was way way too lazy to become a real detective. But let's make no mistake, like any other detective he loves solving mysteries. Only problem is that he always hated rain snow or cold, and preferred to be home cat. 

He was the Sherlock Holmes of pen, paper and few lines of computer code. Since he's an unusal detective, his clients were too unusal. They vary from those who lost a lot of money in gambling and want to prove casinos are cheating, to high profile scientists tring to decide when to launch a spacecraft, or football coaches looking for a new defensive midfielder but not trusting his own scouts. 


It was a another miserable morning, rain never stopped for a moment since he woke up. That's one of the days you want to spend under a blanket and do simply nothing but only sleep. Nevertheless he came to office and now he was enjoying his brewed tea and bagel in his tiny rooftop office. Brewing his tea is one of the few things he was never lazy about, as he always hated bagged teas. 

After finishing his modest breakfast, he briefly checked papers. They were all telling about two airline crashes in three days. It already caused many conspiracy theories. He didn't take it immediately serious as he knows that also papers tell him Mars' gravity would change his love life but he knows the bagel he just ate two mins ago has more gravitational force than Mars, considering the distance between Mars and him and Bagel and him. Then he went back to the newspaper article, thinking having plane crashes in three days apart is possibly very normal, just he needs to look at plance crash data. Nothing to worry about he said. But he decided to check it later when he has some free time from laziness. 

He couldn't finish reading the rest of the newspaper as the rain sound and warm office seat made him sleepy.

After a while, his door was knocked and a frustrated looking man came in, detective woke up briefly, hoping this frustrated man is just a dream and not a client, he preferes sleep to frustrated looking clients. 

It always takes a min or two for him to wake up. 

After a brief silence, detective shows some life sign and started cleaning last bit of sesame seeds in his teeth. He enjoys bagel but sesam always stuck his teeth:

\- yes?...Mr..?

\- Is this the bayesian detective bureau? asked frustated looking man.  

\- ...

\- Not sure I'm in the correct place, Errmm..I'm looking for investigator Independent Variable. 

Yes, that's exactly what was written in his business card, *Mr. Independent Variable - Private Investigator*

\- Yes, it's ... me, 

while waking up. frustrated man didn't wait for detective to ask why he was here and immediately went into the subject: 

\- My name is , well, doesn't really matter.. I am an investigator from a regulatory body to make sure casinos in the country are relatively fair to customers, well... at least they don't do something hidden we don't know. There is this particular casino with a bit dodgy name called *Casino Trust* and they run basically a simple game very similar to a coin flip. We are receiving a lot of complaints from the casino customers that the digital coin flip is biased. I went yesterday to casino and they provided me the last 50000 flip results, and honestly it looks coin flip is fair to me. Almost half of the flips was tails and and the other half was heads. I cannot really say that it is a biased game. However we have excessive amount of complaints from clients so we have to really dig deep. I am wondering detective, can you also have a look at it? 

Our detective was already awake after he heard coin flips. Feed him with questions about coin flips or dice games and you get his attenton immediately. He turned on his PC and started looking at the coin flips data:

```{r, include=FALSE}
library(reticulate)
use_python("/Users/mya03/dev/junk/venv3/bin/python3")

set.seed(42)

# Function reference: https://stephens999.github.io/fiveMinuteStats/simulating_discrete_chains_1.html

run.mc.sim <- function( P, num.iters = 50 ) {
    
    # number of possible states
    num.states <- nrow(P)
    
    # stores the states X_t through time
    states     <- numeric(num.iters)
    
    # initialize variable for first state 
    states[1]    <- 1
    
    for(t in 2:num.iters) {
        
        # probability vector to simulate next state X_{t+1}
        p  <- P[states[t-1], ]
        
        ## draw from multinomial and determine state
        states[t] <-  which(rmultinom(1, 1, p) == 1)
    }
    return(states)
}

# Markov Chain Transition Matrix
p<- matrix(c( 0.6, 0.4, 
                0.4, 0.6), nrow=2, ncol=2)

casino <- run.mc.sim(p,num.iters = 50000) -1 
```


```{python, include=FALSE}
import numpy as np

dataset  = np.array([int(x) for x in r.casino])

```

In the dataset 0 represents **tails** and 1 represent **heads** said no name gentleman.

```{python}
print(dataset)
```

It has 50000 games :
```{python}
len(dataset)
```

And as the no name gentlemen said, the $P(H)$ is pretty close to $0.5$ which indeed looks like a fair coin

```{python}
dataset.mean()
```


Detective puzzled a bit for a moment, as game results seems pretty regular and fair. After all it's hard to blame the casino for not being fair if the 50000 game if dataset shows $P(HEADS)$ ~ $P(TAILS)$ ~ $0.5$

Detective draw a quick diagram on the paper, as he loves to think visually and pen and paper are generally the best tools while thinking on the problem:

- This is a state diagram of a fair coin, said detective. And in a fair coin getting HEADS in the next game is independent from the outcome of the previous game, $P(HEADS|TAILS) = P(HEADS|HEADS) = 0.5$ meaning probability events are independent. 

```{r, include=FALSE, fig.width=5, fig.height=5}
# library(diagram)
# p_fair<- matrix(c( 0.5, 0.5,
#                 0.5, 0.5), nrow=2, ncol=2)
# plotmat(p_fair, relsize = 0.6, lwd=2, name=names, box.prop = 0.9, curve = 0.01, self.cex = 0.6)
```
![](/blog/2019-06-04-is-it-really-a-fair-coin.en_files/Screen Shot 2019-06-04 at 15.05.46.png)


He looked a bit closer to the diagram, and started talking to himself as he always does when he is focused,

\- Himm, we know at least $P(HEADS|TAILS) + P(HEADS|HEADS) =  P(TAILS|HEADS) + P(TAILS|TAILS)$.  This guarantees we have $P(HEADS) = P(TAILS) = 0.5$
  and if we assume coin is fair then
  
- $P(HEADS|TAILS) = 0.5 \times 0.5 = 0.25$ 
- $P(HEADS|HEADS) = 0.5 \times 0.5 = 0.25$ 
- $P(TAILS|HEADS) = 0.5 \times 0.5 = 0.25$
- $P(TAILS|TAILS) = 0.5 \times 0.5 = 0.25$

The gentlemen was trying to follow the detective, thinking about independency of coin flips. Right at that point he remembered customer complaints. Number of customers said they believed if the previous game was HEADS, they think the next game would be more likely HEADS too. 

\- Is it?! detective shouted. You are telling me events are not independent? That changes our assumption, himm. Gentleman puzzled :
\- I don't get it fully, if the events are not independent how is it possible $P(HEADS) = P(TAILS) = 0.5$?

Detective *Independent Variable* was looking excited, and rewote the assumptions:

We all need to satisfy only one condition to keep $P(HEADS) = P(TAILS) = 0.5$.Transition probabilites must be equal:

 $$P(HEADS|TAILS) = P(TAILS|HEADS)$$

and let's say if our diagram changes, and the game outcome becomes stickier, meaning if previous result is HEADS this game result is also more likely be HEADS, and keeping this symmetry would give us a $P(HEADS) = P(TAILS) = 0.5$. Just our dataset would have different number of consecutive same faces. If the game becomes stickier, than we would see more consecutive series like (TAIL>TAIL>TAIL) than usual.  

![Stickier Transition State Diagram](/blog/2019-06-04-is-it-really-a-fair-coin.en_files/Screen Shot 2019-06-04 at 15.28.28.png)

Gentleman started to get the idea now. But still he has questions:

\- I think got what you mean, but how can we prove this is the case?

Detective thought a min, and said:

\- I think there are number of ways we can prove it. But I'd like to explore both visually and analitically. In a fair coin case the possibility of transitioning from one state to another (i.e. HEADS > TAILS > HEADS  or TAILS > HEADS > TAILS) is =  $0.5(transition in) \times 0.5(transition out) = 0.25$. similarly having two consecutive same face is  $0.5 (transition) \times 0.5 (same face) \times 0.5 (transition out) = 0.125$ as all these events are independent. If we plot the histogram of coins series where we get the same coin consutively in a fair coin we should see this diminishing factor of $0.5$ with each number. In our loaded coin example this would be different. Diminishing factor would be different and more gradual as it's more sticky. Transitioning from one state to another would be $0.1(transition in) \times 0.1(transition out) = 0.01$  and two conscutive same faces would be $0.1(transition in) \times 0.9(same face) \times 0.1(transition out) = 0.009$.


In general formula for $n$ number of consecutive faces would be; $$P(n\ Consecutive\ faces) = P(transition\ in) \times P(same\ face)^{(n-1)} \times P(transition\ out)$$


Assuming $P(transition\ in) = P(transition\ out)$ our formula would be $$P(n\ Consecutive\ faces) = P(transition)^2 \times P(same\ face)^{(n-1)} $$


I'll generate an artifical 50000 fair coin flips to see it in action:

```{python}
import numpy as np
np.random.seed(42)

fair_coin = np.random.randint(0,2, 50000)
fair_coin[:20]
```

above series would be summarized as  $1T, 1H, 3T, 1H, 3T, 1H, 4T...$ etc 

I will create exactly that series above. 

```{python}
from itertools import groupby
series= np.array([len(list(x)) for key, x in groupby(fair_coin)])
series[:10]
```

Let's look at the histogram of consecutive series:

```{r, warning=FALSE, message=FALSE}
hist(py$series, bins= max(py$series) -1, xlab = "Consecutive series", 
     ylab = "Number of occurences", main = "Histogram of a fair coin consecutive same face occurences")
```

Let's also calculate the ratio of consequtive series, remember 1 consecutive means TAILS>TAILS or HEADS>HEADS.  and $P(H|T)\times P(H) \times P(T|H) = 0.125$

```{r}
table(py$series)/50000
```


Our detective continued talking:

\- let's look at our not so fair coin model that depends on the previous state (current state depends only the last state not ALL previous states)

![Sticky coin, P(TAILS|TAILS) = P(HEADS|HEADS) = 0.9](/blog/2019-06-04-is-it-really-a-fair-coin.en_files/Screen Shot 2019-06-05 at 09.43.53.png)

```{r, include=FALSE}
p_unfair <- matrix(c( 0.9, 0.1, 
                      0.1, 0.9), nrow=2, ncol=2)

unfair <- run.mc.sim(p_unfair, num.iters = 50000) -1
```

```{r}
unfair[1:50]
length(unfair)
mean(unfair)
```


```{python}
from itertools import groupby
unfair_series= np.array([len(list(x)) for key, x in groupby(np.array(r.unfair))])
unfair_series[:10]
```


```{r warning=FALSE, message=FALSE}
table(py$unfair_series)/50000
# 
hist(py$unfair_series, bins= max(py$unfair_series) -1, xlab = "Consecutive series",
     ylab = "Number of occurences in unfair coin", main = "Histogram of am unfair (with memory) coin consecutive same face occurences")
```

After looking at the result of histogram and the table, no name frustrated looking gentleman grasped the idea. He was no longer frustrated looking no name gentleman. Excitingly he said:
 - Right! Makes very much sense now. I think we can even quantify the transition matrix for the casino coin! All we need to take the square root of the first in the table. That should give us the probaility of transitioning from HEADS to TAILS or TAILS to HEADS. If it is different than 0.5 we know it's not a fair coin and we can find how sticky it is.
 
Detective nodded his head:

\- Exactly! This is interesting since the coin is both fair and unfair at the same time. It is a ***HeisenCoin***. I coined this term! Anyways, shall we look at the casino dataset? 


Satisfied looking no name gentleman was not very happy with this silly joke but didn't want to say it outloud. Simply smiled and agreed to look at casino data. Detective loaded the casino dataset for analysis:

```{python}
dataset[:20]
casino_series = np.array([len(list(x)) for key, x in groupby(dataset)])
casino_series[:10]
```

```{r}
table(py$casino_series)/50000
```

\- The first term is $0.16$, this means we indeed DO have an unfair game and $P(HEAD|TAILS)$ or $P(TAILS|HEAD) = 0.4$. Detective draw transition state diagram on a piece of paper. 

![Casino Coin Transition State Diagram](/blog/2019-06-04-is-it-really-a-fair-coin.en_files/Screen Shot 2019-06-05 at 10.23.08.png)


Gentleman was thrilled:
\ -  Yes indeed! The second term, two consecutive faces TAILS>TAILS or HEADS>HEADS must be then $P(2) = P(transition)^2 \times P(same\ face) =  0.4^2 \times 0.6 = 0.096$ and indeed true, our simulation result is$ $0.9632$! Three consecutive faces must be $P(3) = 0.4^2 \times 0.6^2 = 0.0576$ very close to our simulation findings of $0.597$. Thank you detective! you are the best! 
 
Our detective was going to give a bit more information but thrilled looking no name gentleman was in hurry, as he was running to quit. Detective just said one last sentence before he leaves the office:

\- Next time if you are suspicious that an event depends on its immediate previous state then check out Markov Chains first. and ... yes you are welcome...




#### NOTES

A simple python code that populates an unfair (sticky) markov coin:

```
import random
import numpy as np 

def markov_coin(n, p=0.5):
    """ 
    Populate n samples of coin flips with stickiness probability of p
    p = 0.5 is a fair coin, p>0.5 is a sticky coin, p<0.5 is a hot potato coin.
    Sticky coin tends to stay in the same state more, and hot potato coin is eager to 
    change state
    """
    
    # Initialize list
    arr = list()
    arr.append(random.random()>0.5)
    
    for i in range(1,n):
        if random.random() < p:
            arr.append(arr[i-1])
        else:
            arr.append(not arr[i-1])
    return np.array(arr)
    
```


Markov chain model in `R`. Slightly more complete example compared to pythona s it takes transition matrix of $P$

```
# Function reference: https://stephens999.github.io/fiveMinuteStats/simulating_discrete_chains_1.html

run.mc.sim <- function(P, num.iters = 50 ) {
    
    # number of possible states
    num.states <- nrow(P)
    
    # stores the states X_t through time
    states     <- numeric(num.iters)
    
    # initialize variable for first state 
    states[1]    <- 1
    
    for(t in 2:num.iters) {
        
        # probability vector to simulate next state X_{t+1}
        p  <- P[states[t-1], ]
        
        ## draw from multinomial and determine state
        states[t] <-  which(rmultinom(1, 1, p) == 1)
    }
    return(states)
}

# Markov Chain Transition Matrix
P<- matrix(c( 0.6, 0.4, 
              0.4, 0.6), nrow=2, ncol=2)
```








