---
title: 'Is it really a fair coin? '
author: Mehmet Oner Yalcin
date: '2019-06-04'
slug: is-it-really-a-fair-coin (Bayesian Detective Bureau)
categories:
  - Probability
tags:
  - R
description: ''
featured: ''
featuredalt: ''
featuredpath: ''
linktitle: ''
type: post
---



<p>Unlike any other detectives, he was not not very much into chasing leads out in the wild. He never enjoyed running after serial killers, or going into adventures like Indiana Jones. He was way way too lazy to become a real detective. But let’s make no mistake, like any other detective he loves solving mysteries. Only problem is that he always hated rain snow or cold, and preferred to be home cat.</p>
<p>He was the Sherlock Holmes of pen, paper and few lines of computer code. Since he’s an unusal detective, his clients were too unusal. They vary from those who lost a lot of money in gambling and want to prove casinos are cheating, to high profile scientists tring to decide when to launch a spacecraft, or football coaches looking for a new defensive midfielder but not trusting his own scouts.</p>
<p>It was a another miserable morning, rain never stopped for a moment since he woke up. That’s one of the days you want to spend under a blanket and do simply nothing but only sleep. Nevertheless he came to office and now he was enjoying his brewed tea and bagel in his tiny rooftop office. Brewing his tea is one of the few things he was never lazy about, as he always hated bagged teas.</p>
<p>After finishing his modest breakfast, he briefly checked papers. They were all telling about two airline crashes in three days. It already caused many conspiracy theories. He didn’t take it immediately serious as he knows that also papers tell him Mars’ gravity would change his love life but he knows the bagel he just ate two mins ago has more gravitational force than Mars, considering the distance between Mars and him and Bagel and him. Then he went back to the newspaper article, thinking having plane crashes in three days apart is possibly very normal, just he needs to look at plance crash data. Nothing to worry about he said. But he decided to check it later when he has some free time from laziness.</p>
<p>He couldn’t finish reading the rest of the newspaper as the rain sound and warm office seat made him sleepy.</p>
<p>After a while, his door was knocked and a frustrated looking man came in, detective woke up briefly, hoping this frustrated man is just a dream and not a client, he preferes sleep to frustrated looking clients.</p>
<p>It always takes a min or two for him to wake up.</p>
<p>After a brief silence, detective shows some life sign and started cleaning last bit of sesame seeds in his teeth. He enjoys bagel but sesam always stuck his teeth:</p>
<p>- yes?…Mr..?</p>
<p>- Is this the bayesian detective bureau? asked frustated looking man.</p>
<p>- …</p>
<p>- Not sure I’m in the correct place, Errmm..I’m looking for investigator Independent Variable.</p>
<p>Yes, that’s exactly what was written in his business card, <em>Mr. Independent Variable - Private Investigator</em></p>
<p>- Yes, it’s … me,</p>
<p>while waking up. frustrated man didn’t wait for detective to ask why he was here and immediately went into the subject:</p>
<p>- My name is , well, doesn’t really matter.. I am an investigator from a regulatory body to make sure casinos in the country are relatively fair to customers, well… at least they don’t do something hidden we don’t know. There is this particular casino with a bit dodgy name called <em>Casino Trust</em> and they run basically a simple game very similar to a coin flip. We are receiving a lot of complaints from the casino customers that the digital coin flip is biased. I went yesterday to casino and they provided me the last 50000 flip results, and honestly it looks coin flip is fair to me. Almost half of the flips was tails and and the other half was heads. I cannot really say that it is a biased game. However we have excessive amount of complaints from clients so we have to really dig deep. I am wondering detective, can you also have a look at it?</p>
<p>Our detective was already awake after he heard coin flips. Feed him with questions about coin flips or dice games and you get his attenton immediately. He turned on his PC and started looking at the coin flips data:</p>
<p>In the dataset 0 represents <strong>tails</strong> and 1 represent <strong>heads</strong> said no name gentleman.</p>
<pre class="python"><code>print(dataset)</code></pre>
<pre><code>## [0 1 0 ... 0 0 0]</code></pre>
<p>It has 50000 games :</p>
<pre class="python"><code>len(dataset)</code></pre>
<pre><code>## 50000</code></pre>
<p>And as the no name gentlemen said, the <span class="math inline">\(P(H)\)</span> is pretty close to <span class="math inline">\(0.5\)</span> which indeed looks like a fair coin</p>
<pre class="python"><code>dataset.mean()</code></pre>
<pre><code>## 0.5002</code></pre>
<p>Detective puzzled a bit for a moment, as game results seems pretty regular and fair. After all it’s hard to blame the casino for not being fair if the 50000 game if dataset shows <span class="math inline">\(P(HEADS)\)</span> ~ <span class="math inline">\(P(TAILS)\)</span> ~ <span class="math inline">\(0.5\)</span></p>
<p>Detective draw a quick diagram on the paper, as he loves to think visually and pen and paper are generally the best tools while thinking on the problem:</p>
<ul>
<li>This is a state diagram of a fair coin, said detective. And in a fair coin getting HEADS in the next game is independent from the outcome of the previous game, <span class="math inline">\(P(HEADS|TAILS) = P(HEADS|HEADS) = 0.5\)</span> meaning probability events are independent.</li>
</ul>
<p><img src="/blog/2019-06-04-is-it-really-a-fair-coin.en_files/Screen%20Shot%202019-06-04%20at%2015.05.46.png" /></p>
<p>He looked a bit closer to the diagram, and started talking to himself as he always does when he is focused,</p>
<p>- Himm, we know at least <span class="math inline">\(P(HEADS|TAILS) + P(HEADS|HEADS) = P(TAILS|HEADS) + P(TAILS|TAILS)\)</span>. This guarantees we have <span class="math inline">\(P(HEADS) = P(TAILS) = 0.5\)</span>
and if we assume coin is fair then</p>
<ul>
<li><span class="math inline">\(P(HEADS|TAILS) = 0.5 \times 0.5 = 0.25\)</span></li>
<li><span class="math inline">\(P(HEADS|HEADS) = 0.5 \times 0.5 = 0.25\)</span></li>
<li><span class="math inline">\(P(TAILS|HEADS) = 0.5 \times 0.5 = 0.25\)</span></li>
<li><span class="math inline">\(P(TAILS|TAILS) = 0.5 \times 0.5 = 0.25\)</span></li>
</ul>
<p>The gentlemen was trying to follow the detective, thinking about independency of coin flips. Right at that point he remembered customer complaints. Number of customers said they believed if the previous game was HEADS, they think the next game would be more likely HEADS too.</p>
<p>- Is it?! detective shouted. You are telling me events are not independent? That changes our assumption, himm. Gentleman puzzled :
- I don’t get it fully, if the events are not independent how is it possible <span class="math inline">\(P(HEADS) = P(TAILS) = 0.5\)</span>?</p>
<p>Detective <em>Independent Variable</em> was looking excited, and rewote the assumptions:</p>
<p>We all need to satisfy only one condition to keep <span class="math inline">\(P(HEADS) = P(TAILS) = 0.5\)</span>.Transition probabilites must be equal:</p>
<p><span class="math display">\[P(HEADS|TAILS) = P(TAILS|HEADS)\]</span></p>
<p>and let’s say if our diagram changes, and the game outcome becomes stickier, meaning if previous result is HEADS this game result is also more likely be HEADS, and keeping this symmetry would give us a <span class="math inline">\(P(HEADS) = P(TAILS) = 0.5\)</span>. Just our dataset would have different number of consecutive same faces. If the game becomes stickier, than we would see more consecutive series like (TAIL&gt;TAIL&gt;TAIL) than usual.</p>
<div class="figure">
<img src="/blog/2019-06-04-is-it-really-a-fair-coin.en_files/Screen%20Shot%202019-06-04%20at%2015.28.28.png" alt="Stickier Transition State Diagram" />
<p class="caption">Stickier Transition State Diagram</p>
</div>
<p>Gentleman started to get the idea now. But still he has questions:</p>
<p>- I think got what you mean, but how can we prove this is the case?</p>
<p>Detective thought a min, and said:</p>
<p>- I think there are number of ways we can prove it. But I’d like to explore both visually and analitically. In a fair coin case the possibility of transitioning from one state to another (i.e. HEADS &gt; TAILS &gt; HEADS or TAILS &gt; HEADS &gt; TAILS) is = <span class="math inline">\(0.5(transition in) \times 0.5(transition out) = 0.25\)</span>. similarly having two consecutive same face is <span class="math inline">\(0.5 (transition) \times 0.5 (same face) \times 0.5 (transition out) = 0.125\)</span> as all these events are independent. If we plot the histogram of coins series where we get the same coin consutively in a fair coin we should see this diminishing factor of <span class="math inline">\(0.5\)</span> with each number. In our loaded coin example this would be different. Diminishing factor would be different and more gradual as it’s more sticky. Transitioning from one state to another would be <span class="math inline">\(0.1(transition in) \times 0.1(transition out) = 0.01\)</span> and two conscutive same faces would be <span class="math inline">\(0.1(transition in) \times 0.9(same face) \times 0.1(transition out) = 0.009\)</span>.</p>
<p>In general formula for <span class="math inline">\(n\)</span> number of consecutive faces would be; <span class="math display">\[P(n\ Consecutive\ faces) = P(transition\ in) \times P(same\ face)^{(n-1)} \times P(transition\ out)\]</span></p>
<p>Assuming <span class="math inline">\(P(transition\ in) = P(transition\ out)\)</span> our formula would be <span class="math display">\[P(n\ Consecutive\ faces) = P(transition)^2 \times P(same\ face)^{(n-1)} \]</span></p>
<p>I’ll generate an artifical 50000 fair coin flips to see it in action:</p>
<pre class="python"><code>import numpy as np
np.random.seed(42)

fair_coin = np.random.randint(0,2, 50000)
fair_coin[:20]</code></pre>
<pre><code>## array([0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0])</code></pre>
<p>above series would be summarized as <span class="math inline">\(1T, 1H, 3T, 1H, 3T, 1H, 4T...\)</span> etc</p>
<p>I will create exactly that series above.</p>
<pre class="python"><code>from itertools import groupby
series= np.array([len(list(x)) for key, x in groupby(fair_coin)])
series[:10]</code></pre>
<pre><code>## array([1, 1, 3, 1, 3, 1, 4, 1, 1, 3])</code></pre>
<p>Let’s look at the histogram of consecutive series:</p>
<pre class="r"><code>hist(py$series, bins= max(py$series) -1, xlab = &quot;Consecutive series&quot;, 
     ylab = &quot;Number of occurences&quot;, main = &quot;Histogram of a fair coin consecutive same face occurences&quot;)</code></pre>
<p><img src="/blog/2019-06-04-is-it-really-a-fair-coin.en_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>Let’s also calculate the ratio of consequtive series, remember 1 consecutive means TAILS&gt;TAILS or HEADS&gt;HEADS. and <span class="math inline">\(P(H|T)\times P(H) \times P(T|H) = 0.125\)</span></p>
<pre class="r"><code>table(py$series)/50000</code></pre>
<pre><code>## 
##       1       2       3       4       5       6       7       8       9 
## 0.24648 0.12470 0.06130 0.03160 0.01666 0.00818 0.00396 0.00194 0.00108 
##      10      11      12      13      16 
## 0.00034 0.00018 0.00014 0.00006 0.00004</code></pre>
<p>Our detective continued talking:</p>
<p>- let’s look at our not so fair coin model that depends on the previous state (current state depends only the last state not ALL previous states)</p>
<div class="figure">
<img src="/blog/2019-06-04-is-it-really-a-fair-coin.en_files/Screen%20Shot%202019-06-05%20at%2009.43.53.png" alt="Sticky coin, P(TAILS|TAILS) = P(HEADS|HEADS) = 0.9" />
<p class="caption">Sticky coin, P(TAILS|TAILS) = P(HEADS|HEADS) = 0.9</p>
</div>
<pre class="r"><code>unfair[1:50]</code></pre>
<pre><code>##  [1] 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1
## [36] 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0</code></pre>
<pre class="r"><code>length(unfair)</code></pre>
<pre><code>## [1] 50000</code></pre>
<pre class="r"><code>mean(unfair)</code></pre>
<pre><code>## [1] 0.49546</code></pre>
<pre class="python"><code>from itertools import groupby
unfair_series= np.array([len(list(x)) for key, x in groupby(np.array(r.unfair))])
unfair_series[:10]</code></pre>
<pre><code>## array([ 9,  7,  1,  1, 11, 14,  7,  2,  1, 17])</code></pre>
<pre class="r"><code>table(py$unfair_series)/50000</code></pre>
<pre><code>## 
##       1       2       3       4       5       6       7       8       9 
## 0.01030 0.00900 0.00818 0.00748 0.00646 0.00610 0.00490 0.00462 0.00400 
##      10      11      12      13      14      15      16      17      18 
## 0.00394 0.00366 0.00352 0.00316 0.00294 0.00222 0.00174 0.00176 0.00134 
##      19      20      21      22      23      24      25      26      27 
## 0.00148 0.00122 0.00130 0.00100 0.00094 0.00104 0.00092 0.00066 0.00082 
##      28      29      30      31      32      33      34      35      36 
## 0.00072 0.00050 0.00038 0.00026 0.00034 0.00034 0.00038 0.00026 0.00024 
##      37      38      39      40      41      42      43      44      45 
## 0.00032 0.00012 0.00012 0.00014 0.00020 0.00014 0.00016 0.00008 0.00010 
##      46      47      48      49      50      51      52      53      54 
## 0.00012 0.00002 0.00004 0.00008 0.00008 0.00002 0.00006 0.00002 0.00004 
##      55      56      57      58      59      60      62      64      67 
## 0.00006 0.00012 0.00004 0.00002 0.00004 0.00004 0.00002 0.00002 0.00002 
##      68      71 
## 0.00002 0.00002</code></pre>
<pre class="r"><code># 
hist(py$unfair_series, bins= max(py$unfair_series) -1, xlab = &quot;Consecutive series&quot;,
     ylab = &quot;Number of occurences in unfair coin&quot;, main = &quot;Histogram of am unfair (with memory) coin consecutive same face occurences&quot;)</code></pre>
<p><img src="/blog/2019-06-04-is-it-really-a-fair-coin.en_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>After looking at the result of histogram and the table, no name frustrated looking gentleman grasped the idea. He was no longer frustrated looking no name gentleman. Excitingly he said:
- Right! Makes very much sense now. I think we can even quantify the transition matrix for the casino coin! All we need to take the square root of the first in the table. That should give us the probaility of transitioning from HEADS to TAILS or TAILS to HEADS. If it is different than 0.5 we know it’s not a fair coin and we can find how sticky it is.</p>
<p>Detective nodded his head:</p>
<p>- Exactly! This is interesting since the coin is both fair and unfair at the same time. It is a <strong><em>HeisenCoin</em></strong>. I coined this term! Anyways, shall we look at the casino dataset?</p>
<p>Satisfied looking no name gentleman was not very happy with this silly joke but didn’t want to say it outloud. Simply smiled and agreed to look at casino data. Detective loaded the casino dataset for analysis:</p>
<pre class="python"><code>dataset[:20]</code></pre>
<pre><code>## array([0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1])</code></pre>
<pre class="python"><code>casino_series = np.array([len(list(x)) for key, x in groupby(dataset)])
casino_series[:10]</code></pre>
<pre><code>## array([1, 1, 2, 1, 2, 2, 1, 2, 1, 3])</code></pre>
<pre class="r"><code>table(py$casino_series)/50000</code></pre>
<pre><code>## 
##       1       2       3       4       5       6       7       8       9 
## 0.16004 0.09632 0.05970 0.03414 0.01988 0.01250 0.00746 0.00442 0.00284 
##      10      11      12      13      14      15      16      17      18 
## 0.00170 0.00082 0.00054 0.00032 0.00028 0.00008 0.00006 0.00006 0.00002</code></pre>
<p>- The first term is <span class="math inline">\(0.16\)</span>, this means we indeed DO have an unfair game and <span class="math inline">\(P(HEAD|TAILS)\)</span> or <span class="math inline">\(P(TAILS|HEAD) = 0.4\)</span>. Detective draw transition state diagram on a piece of paper.</p>
<div class="figure">
<img src="/blog/2019-06-04-is-it-really-a-fair-coin.en_files/Screen%20Shot%202019-06-05%20at%2010.23.08.png" alt="Casino Coin Transition State Diagram" />
<p class="caption">Casino Coin Transition State Diagram</p>
</div>
<p>Gentleman was thrilled:
 - Yes indeed! The second term, two consecutive faces TAILS&gt;TAILS or HEADS&gt;HEADS must be then <span class="math inline">\(P(2) = P(transition)^2 \times P(same\ face) = 0.4^2 \times 0.6 = 0.096\)</span> and indeed true, our simulation result is$ <span class="math inline">\(0.9632\)</span>! Three consecutive faces must be <span class="math inline">\(P(3) = 0.4^2 \times 0.6^2 = 0.0576\)</span> very close to our simulation findings of <span class="math inline">\(0.597\)</span>. Thank you detective! you are the best!</p>
<p>Our detective was going to give a bit more information but thrilled looking no name gentleman was in hurry, as he was running to quit. Detective just said one last sentence before he leaves the office:</p>
<p>- Next time if you are suspicious that an event depends on its immediate previous state then check out Markov Chains first. and … yes you are welcome…</p>
<div id="notes" class="section level4">
<h4>NOTES</h4>
<p>A simple python code that populates an unfair (sticky) markov coin:</p>
<pre><code>import random
import numpy as np 

def markov_coin(n, p=0.5):
    &quot;&quot;&quot; 
    Populate n samples of coin flips with stickiness probability of p
    p = 0.5 is a fair coin, p&gt;0.5 is a sticky coin, p&lt;0.5 is a hot potato coin.
    Sticky coin tends to stay in the same state more, and hot potato coin is eager to 
    change state
    &quot;&quot;&quot;
    
    # Initialize list
    arr = list()
    arr.append(random.random()&gt;0.5)
    
    for i in range(1,n):
        if random.random() &lt; p:
            arr.append(arr[i-1])
        else:
            arr.append(not arr[i-1])
    return np.array(arr)
    </code></pre>
<p>Markov chain model in <code>R</code>. Slightly more complete example compared to pythona s it takes transition matrix of <span class="math inline">\(P\)</span></p>
<pre><code># Function reference: https://stephens999.github.io/fiveMinuteStats/simulating_discrete_chains_1.html

run.mc.sim &lt;- function(P, num.iters = 50 ) {
    
    # number of possible states
    num.states &lt;- nrow(P)
    
    # stores the states X_t through time
    states     &lt;- numeric(num.iters)
    
    # initialize variable for first state 
    states[1]    &lt;- 1
    
    for(t in 2:num.iters) {
        
        # probability vector to simulate next state X_{t+1}
        p  &lt;- P[states[t-1], ]
        
        ## draw from multinomial and determine state
        states[t] &lt;-  which(rmultinom(1, 1, p) == 1)
    }
    return(states)
}

# Markov Chain Transition Matrix
P&lt;- matrix(c( 0.6, 0.4, 
              0.4, 0.6), nrow=2, ncol=2)</code></pre>
</div>
